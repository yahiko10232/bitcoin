{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "2021-1학기 MRC 프로젝트",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yahiko10232/bitcoin/blob/master/2021_1%ED%95%99%EA%B8%B0_MRC_%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q667Uh-w6M4h"
      },
      "source": [
        "1. 구글 드라이브 연동"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZUZnBnDPSJBu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f111c1a-36f1-4e9e-d98b-065a9a573969"
      },
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/') \n",
        "os.chdir('/content/gdrive/My Drive/Colab Notebooks/DFC615') ## 현재 작업 환경으로 설정한 경로를 입력하세요"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gm0mYEUV6Iyf"
      },
      "source": [
        "1-1. Dataset 다운로드"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YtQyy-Pv7YpY",
        "outputId": "220100a9-bb07-42f6-da7d-bd476d3e2152"
      },
      "source": [
        "import json\n",
        "#경로 설정\n",
        "os.chdir('/content/gdrive/My Drive/Colab Notebooks/DFC615')\n",
        "gdrive_path = \"/content/gdrive/My Drive/Colab Notebooks/DFC615\"\n",
        "\n",
        "#데이터셋 다운로드\n",
        "!wget https://korquad.github.io/dataset/KorQuAD_v1.0_train.json\n",
        "!wget https://korquad.github.io/dataset/KorQuAD_v1.0_dev.json\n",
        "\n",
        "#변수 설정\n",
        "output_dir = gdrive_path\n",
        "train_file = os.path.join(gdrive_path, \"KorQuAD_v1.0_train.json\")\n",
        "dev_file = os.path.join(gdrive_path, \"KorQuAD_v1.0_dev.json\")\n",
        "\n",
        "#데이터셋 로드\n",
        "korquad_train = json.load(open(train_file,'r',encoding='utf-8'))    \n",
        "korquad_dev = json.load(open(dev_file,'r',encoding='utf-8'))    \n",
        "\n",
        "#데이터셋 로드\n",
        "print('done')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-06-25 10:43:30--  https://korquad.github.io/dataset/KorQuAD_v1.0_train.json\n",
            "Resolving korquad.github.io (korquad.github.io)... 185.199.108.153, 185.199.109.153, 185.199.110.153, ...\n",
            "Connecting to korquad.github.io (korquad.github.io)|185.199.108.153|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 38527475 (37M) [application/json]\n",
            "Saving to: ‘KorQuAD_v1.0_train.json.3’\n",
            "\n",
            "KorQuAD_v1.0_train. 100%[===================>]  36.74M  45.6MB/s    in 0.8s    \n",
            "\n",
            "2021-06-25 10:43:31 (45.6 MB/s) - ‘KorQuAD_v1.0_train.json.3’ saved [38527475/38527475]\n",
            "\n",
            "--2021-06-25 10:43:32--  https://korquad.github.io/dataset/KorQuAD_v1.0_dev.json\n",
            "Resolving korquad.github.io (korquad.github.io)... 185.199.108.153, 185.199.109.153, 185.199.110.153, ...\n",
            "Connecting to korquad.github.io (korquad.github.io)|185.199.108.153|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3881058 (3.7M) [application/json]\n",
            "Saving to: ‘KorQuAD_v1.0_dev.json.3’\n",
            "\n",
            "KorQuAD_v1.0_dev.js 100%[===================>]   3.70M  21.4MB/s    in 0.2s    \n",
            "\n",
            "2021-06-25 10:43:32 (21.4 MB/s) - ‘KorQuAD_v1.0_dev.json.3’ saved [3881058/3881058]\n",
            "\n",
            "done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UFcwlbPv8acf",
        "outputId": "3ae809d6-b2a8-4b28-ff0c-96d94501a77d"
      },
      "source": [
        "print(korquad_train.keys())\n",
        "#print(korquad_train.values()) # 로딩 시간이 조금 걸려요\n",
        "\n",
        "## 0번째 타이틀에 대한 지문 집합\n",
        "print(korquad_train['version'])\n",
        "print(len(korquad_train['data']))\n",
        "print('-'*40)\n",
        "\n",
        "print(korquad_train['data'][0].keys()) # title, paragraphs (위키피디아를 기준으로 제작한 데이터임을 고려하기!)\n",
        "print(f\"title: {korquad_train['data'][0]['title']}\") # 제목은 하나 \n",
        "print('-'*40)\n",
        "print('paragraphs')\n",
        "print(korquad_train['data'][0]['paragraphs']) \n",
        "print(len(korquad_train['data'][0]['paragraphs'])) # 지문은 N개\n",
        "print('-'*40)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_keys(['version', 'data'])\n",
            "KorQuAD_v1.0_train\n",
            "1420\n",
            "----------------------------------------\n",
            "dict_keys(['paragraphs', 'title'])\n",
            "title: 파우스트_서곡\n",
            "----------------------------------------\n",
            "paragraphs\n",
            "[{'qas': [{'answers': [{'text': '교향곡', 'answer_start': 54}], 'id': '6566495-0-0', 'question': '바그너는 괴테의 파우스트를 읽고 무엇을 쓰고자 했는가?'}, {'answers': [{'text': '1악장', 'answer_start': 421}], 'id': '6566495-0-1', 'question': '바그너는 교향곡 작곡을 어디까지 쓴 뒤에 중단했는가?'}, {'answers': [{'text': '베토벤의 교향곡 9번', 'answer_start': 194}], 'id': '6566495-0-2', 'question': '바그너가 파우스트 서곡을 쓸 때 어떤 곡의 영향을 받았는가?'}, {'answers': [{'text': '파우스트', 'answer_start': 15}], 'id': '6566518-0-0', 'question': '1839년 바그너가 교향곡의 소재로 쓰려고 했던 책은?'}, {'answers': [{'text': '합창교향곡', 'answer_start': 354}], 'id': '6566518-0-1', 'question': '파우스트 서곡의 라단조 조성이 영향을 받은 베토벤의 곡은?'}, {'answers': [{'text': '1839', 'answer_start': 0}], 'id': '5917067-0-0', 'question': '바그너가 파우스트를 처음으로 읽은 년도는?'}, {'answers': [{'text': '파리', 'answer_start': 410}], 'id': '5917067-0-1', 'question': '바그너가 처음 교향곡 작곡을 한 장소는?'}, {'answers': [{'text': '드레스덴', 'answer_start': 534}], 'id': '5917067-0-2', 'question': '바그너의 1악장의 초연은 어디서 연주되었는가?'}], 'context': '1839년 바그너는 괴테의 파우스트을 처음 읽고 그 내용에 마음이 끌려 이를 소재로 해서 하나의 교향곡을 쓰려는 뜻을 갖는다. 이 시기 바그너는 1838년에 빛 독촉으로 산전수전을 다 걲은 상황이라 좌절과 실망에 가득했으며 메피스토펠레스를 만나는 파우스트의 심경에 공감했다고 한다. 또한 파리에서 아브네크의 지휘로 파리 음악원 관현악단이 연주하는 베토벤의 교향곡 9번을 듣고 깊은 감명을 받았는데, 이것이 이듬해 1월에 파우스트의 서곡으로 쓰여진 이 작품에 조금이라도 영향을 끼쳤으리라는 것은 의심할 여지가 없다. 여기의 라단조 조성의 경우에도 그의 전기에 적혀 있는 것처럼 단순한 정신적 피로나 실의가 반영된 것이 아니라 베토벤의 합창교향곡 조성의 영향을 받은 것을 볼 수 있다. 그렇게 교향곡 작곡을 1839년부터 40년에 걸쳐 파리에서 착수했으나 1악장을 쓴 뒤에 중단했다. 또한 작품의 완성과 동시에 그는 이 서곡(1악장)을 파리 음악원의 연주회에서 연주할 파트보까지 준비하였으나, 실제로는 이루어지지는 않았다. 결국 초연은 4년 반이 지난 후에 드레스덴에서 연주되었고 재연도 이루어졌지만, 이후에 그대로 방치되고 말았다. 그 사이에 그는 리엔치와 방황하는 네덜란드인을 완성하고 탄호이저에도 착수하는 등 분주한 시간을 보냈는데, 그런 바쁜 생활이 이 곡을 잊게 한 것이 아닌가 하는 의견도 있다.'}, {'qas': [{'answers': [{'text': '한스 폰 뷜로', 'answer_start': 402}], 'id': '6566495-1-0', 'question': '바그너의 작품을 시인의 피로 쓰여졌다고 극찬한 것은 누구인가?'}, {'answers': [{'text': '리스트', 'answer_start': 23}], 'id': '6566495-1-1', 'question': '잊혀져 있는 파우스트 서곡 1악장을 부활시킨 것은 누구인가?'}, {'answers': [{'text': '20루이의 금', 'answer_start': 345}], 'id': '6566495-1-2', 'question': '바그너는 다시 개정된 총보를 얼마를 받고 팔았는가?'}, {'answers': [{'text': '리스트', 'answer_start': 23}], 'id': '6566518-1-0', 'question': '파우스트 교향곡을 부활시킨 사람은?'}, {'answers': [{'text': '한스 폰 뷜로', 'answer_start': 402}], 'id': '6566518-1-1', 'question': '파우스트 교향곡을 피아노 독주용으로 편곡한 사람은?'}, {'answers': [{'text': '리스트', 'answer_start': 23}], 'id': '5917067-1-0', 'question': '1악장을 부활시켜 연주한 사람은?'}, {'answers': [{'text': '한스 폰 뷜로', 'answer_start': 402}], 'id': '5917067-1-1', 'question': '파우스트 교향곡에 감탄하여 피아노곡으로 편곡한 사람은?'}, {'answers': [{'text': '1840년', 'answer_start': 3}], 'id': '5917067-1-2', 'question': '리스트가 바그너와 알게 된 연도는?'}], 'context': '한편 1840년부터 바그너와 알고 지내던 리스트가 잊혀져 있던 1악장을 부활시켜 1852년에 바이마르에서 연주했다. 이것을 계기로 바그너도 이 작품에 다시 관심을 갖게 되었고, 그 해 9월에는 총보의 반환을 요구하여 이를 서곡으로 간추린 다음 수정을 했고 브라이트코프흐 & 헤르텔 출판사에서 출판할 개정판도 준비했다. 1853년 5월에는 리스트가 이 작품이 수정되었다는 것을 인정했지만, 끝내 바그너의 출판 계획은 무산되고 말았다. 이후 1855년에 리스트가 자신의 작품 파우스트 교향곡을 거의 완성하여 그 사실을 바그너에게 알렸고, 바그너는 다시 개정된 총보를 리스트에게 보내고 브라이트코프흐 & 헤르텔 출판사에는 20루이의 금을 받고 팔았다. 또한 그의 작품을 “하나하나의 음표가 시인의 피로 쓰여졌다”며 극찬했던 한스 폰 뷜로가 그것을 피아노 독주용으로 편곡했는데, 리스트는 그것을 약간 변형되었을 뿐이라고 지적했다. 이 서곡의 총보 첫머리에는 파우스트 1부의 내용 중 한 구절을 인용하고 있다.'}, {'qas': [{'answers': [{'text': '주제, 동기', 'answer_start': 70}], 'id': '6566495-2-0', 'question': '서주에는 무엇이 암시되어 있는가?'}, {'answers': [{'text': '제1바이올린', 'answer_start': 148}], 'id': '6566495-2-1', 'question': '첫부분에는 어떤 악기를 사용해 더욱 명확하게 나타내는가?'}, {'answers': [{'text': '소나타 형식', 'answer_start': 272}], 'id': '6566495-2-2', 'question': '주요부는 어떤 형식으로 되어 있는가?'}, {'answers': [{'text': '저음 주제', 'answer_start': 102}], 'id': '6566518-2-0', 'question': '첫 부분의 주요주제를 암시하는 주제는?'}, {'answers': [{'text': 'D장조', 'answer_start': 409}], 'id': '6566518-2-1', 'question': '제2주제의 축소된 재현부의 조성은?'}, {'answers': [{'text': '4/4박자', 'answer_start': 35}], 'id': '5917067-2-0', 'question': '곡이 시작할때의 박자는?'}, {'answers': [{'text': '고뇌와 갈망 동기, 청춘의 사랑 동기', 'answer_start': 115}], 'id': '5917067-2-1', 'question': '이 곡의 주요 주제는?'}, {'answers': [{'text': 'D장조', 'answer_start': 409}], 'id': '5917067-2-2', 'question': '제 2주제에선 무슨 장조로 재현되는가?'}], 'context': '이 작품은 라단조, Sehr gehalten(아주 신중하게), 4/4박자의 부드러운 서주로 서주로 시작되는데, 여기에는 주요 주제, 동기의 대부분이 암시, 예고되어 있다. 첫 부분의 저음 주제는 주요 주제(고뇌와 갈망 동기, 청춘의 사랑 동기)를 암시하고 있으며, 제1바이올린으로 더욱 명확하게 나타난다. 또한 그것을 이어받는 동기도 중요한 역할을 한다. 여기에 새로운 소재가 더해진 뒤에 새로운 주제도 연주된다. 주요부는 Sehr bewegt(아주 격동적으로), 2/2박자의 자유로운 소나타 형식으로 매우 드라마틱한 구상과 유기적인 구성을 하고 있다. 여기에는 지금까지의 주제나 소재 외에도 오보에에 의한 선율과 제2주제를 떠올리게 하는 부차적인 주제가 더해지는데, 중간부에서는 약보3이 중심이 되고 제2주제는 축소된 재현부에서 D장조로 재현된다. 마지막에는 주요 주제를 회상하면서 조용히 마친다.'}]\n",
            "3\n",
            "----------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "D--843QA5OrI",
        "outputId": "803e83b5-7152-479b-d39c-7c7ed1cba373"
      },
      "source": [
        "# 0번째 타이틀의 0번째 지문에 대한 {\"질의응답\": ~~ , \"지문\":   ~~}\n",
        "print(korquad_train['data'][0]['paragraphs'][0].keys()) # qas (question answering), context\n",
        "print(korquad_train['data'][0]['paragraphs'][0]['qas'])\n",
        "print(len(korquad_train['data'][0]['paragraphs'][0]['qas'])) # 질의 응답 쌍도 M개\n",
        "print('-'*40)\n",
        "print('context')\n",
        "print(korquad_train['data'][0]['paragraphs'][0]['context'])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-d6952c1ff5d1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 0번째 타이틀의 0번째 지문에 대한 {\"질의응답\": ~~ , \"지문\":   ~~}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkorquad_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'paragraphs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# qas (question answering), context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkorquad_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'paragraphs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'qas'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkorquad_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'paragraphs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'qas'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 질의 응답 쌍도 M개\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-'\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'korquad_train' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "92DmNUgb5SqB",
        "outputId": "9a500a44-86b7-40b1-ebb0-20f62919162e"
      },
      "source": [
        "print('-'*40)\n",
        "print('Question')\n",
        "print(korquad_train['data'][0]['paragraphs'][0]['qas'][0]['question'])\n",
        "print(\"Answer\")\n",
        "print(korquad_train['data'][0]['paragraphs'][0]['qas'][0]['answers']) \n",
        "print('\\n')\n",
        "print('Question')\n",
        "print(korquad_train['data'][0]['paragraphs'][0]['qas'][1]['question'])\n",
        "print(\"Answer\")\n",
        "print(korquad_train['data'][0]['paragraphs'][0]['qas'][1]['answers']) \n",
        "print('\\n')\n",
        "print('Question')\n",
        "print(korquad_train['data'][0]['paragraphs'][0]['qas'][2]['question'])\n",
        "print(\"Answer\")\n",
        "print(korquad_train['data'][0]['paragraphs'][0]['qas'][2]['answers']) \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------\n",
            "Question\n",
            "바그너는 괴테의 파우스트를 읽고 무엇을 쓰고자 했는가?\n",
            "Answer\n",
            "[{'text': '교향곡', 'answer_start': 54}]\n",
            "\n",
            "\n",
            "Question\n",
            "바그너는 교향곡 작곡을 어디까지 쓴 뒤에 중단했는가?\n",
            "Answer\n",
            "[{'text': '1악장', 'answer_start': 421}]\n",
            "\n",
            "\n",
            "Question\n",
            "바그너가 파우스트 서곡을 쓸 때 어떤 곡의 영향을 받았는가?\n",
            "Answer\n",
            "[{'text': '베토벤의 교향곡 9번', 'answer_start': 194}]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hFOxHl6Ziqtp",
        "outputId": "df0392bf-c3a4-4815-8806-88f008fd050d"
      },
      "source": [
        "train_cnt, val_cnt = 0, 0\n",
        "for idx, train in enumerate(korquad_train['data']):\n",
        "    title_name = train['title']\n",
        "    for paragraph in train['paragraphs']:\n",
        "        context=paragraph['context'] \n",
        "        for qas in paragraph['qas']:\n",
        "            train_cnt += 1\n",
        "\n",
        "for idx, train in enumerate(korquad_dev['data']): \n",
        "    title_name = train['title']\n",
        "    for paragraph in train['paragraphs']:\n",
        "        context=paragraph['context'] \n",
        "        for qas in paragraph['qas']:\n",
        "            val_cnt += 1\n",
        "print(f'Train: {train_cnt} | Val: {val_cnt}')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train: 60407 | Val: 5774\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fpbCKNhk9F1m"
      },
      "source": [
        "## 2. 소스코드\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3pgUZDJ30xuL",
        "outputId": "34aa6a9e-cf9d-44f4-ab22-0ed37fd2d166"
      },
      "source": [
        "# !pip install sentencepiece\n",
        "!pip install transformers==3.3.1\n",
        "!pip install seqeval\n",
        "!pip install fastprogress\n",
        "!pip install attrdict\n",
        "!pip uninstall pandas\n",
        "!pip install pandas==1.1.5\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers==3.3.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/19/22/aff234f4a841f8999e68a7a94bdd4b60b4cebcfeca5d67d61cd08c9179de/transformers-3.3.1-py3-none-any.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 4.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==3.3.1) (2019.12.20)\n",
            "Collecting tokenizers==0.8.1.rc2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/26/c02ba92ecb8b780bdae4a862d351433c2912fe49469dac7f87a5c85ccca6/tokenizers-0.8.1rc2-cp37-cp37m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 17.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==3.3.1) (20.9)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==3.3.1) (1.19.5)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 39.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==3.3.1) (3.0.12)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==3.3.1) (4.41.1)\n",
            "Collecting sentencepiece!=0.1.92\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ac/aa/1437691b0c7c83086ebb79ce2da16e00bef024f24fec2a5161c35476f499/sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 26.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==3.3.1) (2.23.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==3.3.1) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.3.1) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.3.1) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.3.1) (1.0.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.3.1) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.3.1) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.3.1) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.3.1) (3.0.4)\n",
            "Installing collected packages: tokenizers, sacremoses, sentencepiece, transformers\n",
            "Successfully installed sacremoses-0.0.45 sentencepiece-0.1.96 tokenizers-0.8.1rc2 transformers-3.3.1\n",
            "Collecting seqeval\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9d/2d/233c79d5b4e5ab1dbf111242299153f3caddddbb691219f363ad55ce783d/seqeval-1.2.2.tar.gz (43kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 3.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from seqeval) (1.19.5)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from seqeval) (0.22.2.post1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.0.1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.4.1)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-cp37-none-any.whl size=16184 sha256=5182d96bac23c6fb8ac73e9215fe0d71077de28e42ca746b5b22384a336fe686\n",
            "  Stored in directory: /root/.cache/pip/wheels/52/df/1b/45d75646c37428f7e626214704a0e35bd3cfc32eda37e59e5f\n",
            "Successfully built seqeval\n",
            "Installing collected packages: seqeval\n",
            "Successfully installed seqeval-1.2.2\n",
            "Requirement already satisfied: fastprogress in /usr/local/lib/python3.7/dist-packages (1.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fastprogress) (1.19.5)\n",
            "Collecting attrdict\n",
            "  Downloading https://files.pythonhosted.org/packages/ef/97/28fe7e68bc7adfce67d4339756e85e9fcf3c6fd7f0c0781695352b70472c/attrdict-2.0.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from attrdict) (1.15.0)\n",
            "Installing collected packages: attrdict\n",
            "Successfully installed attrdict-2.0.1\n",
            "Uninstalling pandas-1.1.5:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.7/dist-packages/pandas-1.1.5.dist-info/*\n",
            "    /usr/local/lib/python3.7/dist-packages/pandas/*\n",
            "Proceed (y/n)? y\n",
            "  Successfully uninstalled pandas-1.1.5\n",
            "Collecting pandas==1.1.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fd/70/e8eee0cbddf926bf51958c7d6a86bc69167c300fa2ba8e592330a2377d1b/pandas-1.1.5-cp37-cp37m-manylinux1_x86_64.whl (9.5MB)\n",
            "\u001b[K     |████████████████████████████████| 9.5MB 5.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from pandas==1.1.5) (1.19.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas==1.1.5) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas==1.1.5) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas==1.1.5) (1.15.0)\n",
            "Installing collected packages: pandas\n",
            "Successfully installed pandas-1.1.5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pandas"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECEO0b3Z_b3h"
      },
      "source": [
        "## 2.1 필요한 라이브러리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkBcdP03_bYq"
      },
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "from collections import Counter\n",
        "import sys\n",
        "import argparse\n",
        "import string\n",
        "import re\n",
        "import glob\n",
        "import logging\n",
        "import random\n",
        "import timeit\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "from fastprogress.fastprogress import master_bar, progress_bar\n",
        "from attrdict import AttrDict\n",
        "\n",
        "from transformers import (\n",
        "    AdamW,\n",
        "    get_linear_schedule_with_warmup,\n",
        "    squad_convert_examples_to_features\n",
        ")\n",
        "from transformers.data.metrics.squad_metrics import (\n",
        "    compute_predictions_logits,\n",
        "    squad_evaluate,\n",
        ")\n",
        "\n",
        "from transformers import (\n",
        "    ElectraForQuestionAnswering,\n",
        "    XLMRobertaForQuestionAnswering,\n",
        "    \n",
        "    ElectraTokenizer,\n",
        "    XLMRobertaTokenizer,\n",
        "\n",
        "    ElectraConfig,\n",
        "    XLMRobertaConfig\n",
        ")\n",
        "from transformers.data.processors.squad import SquadResult, SquadV1Processor, SquadV2Processor\n",
        "from transformers import pipeline\n",
        "\n",
        "logger = logging.getLogger(__name__)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3WPNgimuaKt"
      },
      "source": [
        "## 2.2 Fine-Tuning을 위한 코드"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZnYl2fFuttPZ"
      },
      "source": [
        "def init_logger():\n",
        "    logging.basicConfig(\n",
        "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
        "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
        "        level=logging.INFO,\n",
        "    )\n",
        "\n",
        "def set_seed(args):\n",
        "    random.seed(args.seed)\n",
        "    np.random.seed(args.seed)\n",
        "    torch.manual_seed(args.seed)\n",
        "    if not args.no_cuda and torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(args.seed)\n",
        "\n",
        "#다른모델 쓰려면 여기를 바꿔야 됨\n",
        "MODEL_FOR_QUESTION_ANSWERING = {\n",
        "    \"koelectra-base-v3\": ElectraForQuestionAnswering,\n",
        "    \"koelectra-small-v3\": ElectraForQuestionAnswering,\n",
        "}\n",
        "TOKENIZER_CLASSES = {\n",
        "    \"koelectra-base-v3\": ElectraTokenizer,\n",
        "    \"koelectra-small-v3\": ElectraTokenizer,\n",
        "}\n",
        "CONFIG_CLASSES = {\n",
        "    \"koelectra-base-v3\": ElectraConfig,\n",
        "    \"koelectra-small-v3\": ElectraConfig,\n",
        "}"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ug0At9pUuIlb"
      },
      "source": [
        "\n",
        "'''KorQuAD v1.0에 대한 공식 평가 스크립트 '''\n",
        "'''본 스크립트는 SQuAD v1.1 평가 스크립트 https://rajpurkar.github.io/SQuAD-explorer/ 를 바탕으로 작성됨.'''\n",
        "\n",
        "#형태소 분석 위해  추가 \n",
        "# tokenizer= Komoran()\n",
        "# pos_table = ['JKS','JKC','JKG','JKO','JKB', 'JKV','JKQ', 'JX', 'JC']  \n",
        "\n",
        "\n",
        "def normalize_answer(s):\n",
        "    def remove_(text):\n",
        "        ''' 불필요한 기호 제거 '''\n",
        "        text = re.sub(\"'\", \" \", text)\n",
        "        text = re.sub('\"', \" \", text)\n",
        "        text = re.sub('《', \" \", text)\n",
        "        text = re.sub('》', \" \", text)\n",
        "        text = re.sub('<', \" \", text)\n",
        "        text = re.sub('>', \" \", text)\n",
        "        text = re.sub('〈', \" \", text)\n",
        "        text = re.sub('〉', \" \", text)\n",
        "        text = re.sub(\"\\(\", \" \", text)\n",
        "        text = re.sub(\"\\)\", \" \", text)\n",
        "        #text = re.sub(r'\\([^)]*\\)', \" \", text)\n",
        "        text = re.sub(\"‘\", \" \", text)\n",
        "        text = re.sub(\"’\", \" \", text)\n",
        "        return text\n",
        "\n",
        "    def white_space_fix(text):\n",
        "        return ' '.join(text.split())\n",
        "\n",
        "    def remove_punc(text):\n",
        "        exclude = set(string.punctuation)\n",
        "        return ''.join(ch for ch in text if ch not in exclude)\n",
        "\n",
        "    def lower(text):\n",
        "        return text.lower()\n",
        "\n",
        "    return white_space_fix(remove_punc(lower(remove_(s))))\n",
        "\n",
        "# def normalize_answer2(s):\n",
        "#     def remove_j(text):\n",
        "#       ''' 조사 제거 '''\n",
        "#       m_text = ''  \n",
        "\n",
        "#       token = tokenizer.pos(text) #형태소 분석    \n",
        "      \n",
        "#       for idx, t in enumerate(token):\n",
        "#         if (t[1] in pos_table) and (len(token) -1 == idx):\n",
        "#           token.pop(idx)\n",
        "#           break\n",
        "#         m_text += ''.join(t[0])\n",
        "\n",
        "#       return m_text\n",
        "\n",
        "#     return remove_j(s)\n",
        "\n",
        "def f1_score(prediction, ground_truth):\n",
        "    prediction_tokens = normalize_answer(prediction).split()\n",
        "    ground_truth_tokens = normalize_answer(ground_truth).split()\n",
        "\n",
        "    # F1 by character\n",
        "    prediction_Char = []\n",
        "    for tok in prediction_tokens:\n",
        "        now = [a for a in tok]\n",
        "        prediction_Char.extend(now)\n",
        "\n",
        "    ground_truth_Char = []\n",
        "    for tok in ground_truth_tokens:\n",
        "        now = [a for a in tok]\n",
        "        ground_truth_Char.extend(now)\n",
        "\n",
        "    common = Counter(prediction_Char) & Counter(ground_truth_Char)\n",
        "    num_same = sum(common.values())\n",
        "    if num_same == 0:\n",
        "        return 0\n",
        "\n",
        "    precision = 1.0 * num_same / len(prediction_Char)\n",
        "    recall = 1.0 * num_same / len(ground_truth_Char)\n",
        "    f1 = (2 * precision * recall) / (precision + recall)\n",
        "\n",
        "    return f1\n",
        "\n",
        "\n",
        "def exact_match_score(prediction, ground_truth):\n",
        "    return (normalize_answer(prediction) == normalize_answer(ground_truth))\n",
        "\n",
        "\n",
        "def metric_max_over_ground_truths(metric_fn, prediction, ground_truths):\n",
        "    scores_for_ground_truths = []\n",
        "    for ground_truth in ground_truths:\n",
        "        score = metric_fn(prediction, ground_truth)\n",
        "        scores_for_ground_truths.append(score)\n",
        "    return max(scores_for_ground_truths)\n",
        "\n",
        "\n",
        "def evaluate(dataset, predictions):\n",
        "    f1 = exact_match = total = 0\n",
        "    for article in dataset:\n",
        "        for paragraph in article['paragraphs']:\n",
        "            for qa in paragraph['qas']:\n",
        "                total += 1\n",
        "                if qa['id'] not in predictions:\n",
        "                    message = 'Unanswered question ' + qa['id'] + \\\n",
        "                              ' will receive score 0.'\n",
        "                    print(message, file=sys.stderr)\n",
        "                    continue\n",
        "                ground_truths = list(map(lambda x: x['text'], qa['answers']))\n",
        "                prediction = predictions[qa['id']]\n",
        "                exact_match += metric_max_over_ground_truths(\n",
        "                    exact_match_score, prediction, ground_truths)\n",
        "                f1 += metric_max_over_ground_truths(\n",
        "                    f1_score, prediction, ground_truths)\n",
        "\n",
        "    exact_match = 100.0 * exact_match / total\n",
        "    f1 = 100.0 * f1 / total\n",
        "    return {'official_exact_match': exact_match, 'official_f1': f1}\n",
        "\n",
        "\n",
        "def eval_during_train(args, step):\n",
        "    expected_version = 'KorQuAD_v1.0'\n",
        "\n",
        "    dataset_file = os.path.join(args.data_dir, args.predict_file)\n",
        "    prediction_file = os.path.join(args.output_dir, 'predictions_{}.json'.format(step))\n",
        "\n",
        "    with open(dataset_file) as dataset_f:\n",
        "        dataset_json = json.load(dataset_f)\n",
        "        read_version = \"_\".join(dataset_json['version'].split(\"_\")[:-1])\n",
        "        if (read_version != expected_version):\n",
        "            print('Evaluation expects ' + expected_version +\n",
        "                  ', but got dataset with ' + read_version,\n",
        "                  file=sys.stderr)\n",
        "        dataset = dataset_json['data']\n",
        "    with open(prediction_file) as prediction_f:\n",
        "        predictions = json.load(prediction_f)\n",
        "\n",
        "    return evaluate(dataset, predictions)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gruobhuH9FME"
      },
      "source": [
        "## 2.3 Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98UVT9r617XF"
      },
      "source": [
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "\n",
        "def train(args, train_dataset, model, tokenizer):\n",
        "    \"\"\" Train the model \"\"\"\n",
        "    train_sampler = RandomSampler(train_dataset)\n",
        "    train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.train_batch_size)\n",
        "\n",
        "    if args.max_steps > 0:\n",
        "        t_total = args.max_steps\n",
        "        args.num_train_epochs = args.max_steps // (len(train_dataloader) // args.gradient_accumulation_steps) + 1\n",
        "    else:\n",
        "        t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n",
        "\n",
        "    # Prepare optimizer and schedule (linear warmup and decay)\n",
        "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
        "    optimizer_grouped_parameters = [\n",
        "        {\n",
        "            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "            \"weight_decay\": args.weight_decay,\n",
        "        },\n",
        "        {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
        "    ]\n",
        "    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer, num_warmup_steps=int(t_total * args.warmup_proportion), num_training_steps=t_total\n",
        "    )\n",
        "\n",
        "    # Check if saved optimizer or scheduler states exist\n",
        "    if os.path.isfile(os.path.join(args.model_name_or_path, \"optimizer.pt\")) and os.path.isfile(\n",
        "            os.path.join(args.model_name_or_path, \"scheduler.pt\")\n",
        "    ):\n",
        "        # Load in optimizer and scheduler states\n",
        "        optimizer.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"optimizer.pt\")))\n",
        "        scheduler.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"scheduler.pt\")))\n",
        "\n",
        "    # Train!\n",
        "    logger.info(\"***** Running training *****\")\n",
        "    logger.info(\"  Num examples = %d\", len(train_dataset))\n",
        "    logger.info(\"  Num Epochs = %d\", args.num_train_epochs)\n",
        "    logger.info(\"  Train batch size per GPU = %d\", args.train_batch_size)\n",
        "    logger.info(\n",
        "        \"  Total train batch size (w. parallel, distributed & accumulation) = %d\",\n",
        "        args.train_batch_size\n",
        "        * args.gradient_accumulation_steps)\n",
        "    logger.info(\"  Gradient Accumulation steps = %d\", args.gradient_accumulation_steps)\n",
        "    logger.info(\"  Total optimization steps = %d\", t_total)\n",
        "\n",
        "    global_step = 1\n",
        "    epochs_trained = 0\n",
        "    steps_trained_in_current_epoch = 0\n",
        "    # Check if continuing training from a checkpoint\n",
        "    if os.path.exists(args.model_name_or_path):\n",
        "        try:\n",
        "            # set global_step to gobal_step of last saved checkpoint from model path\n",
        "            checkpoint_suffix = args.model_name_or_path.split(\"-\")[-1].split(\"/\")[0]\n",
        "            global_step = int(checkpoint_suffix)\n",
        "            epochs_trained = global_step // (len(train_dataloader) // args.gradient_accumulation_steps)\n",
        "            steps_trained_in_current_epoch = global_step % (len(train_dataloader) // args.gradient_accumulation_steps)\n",
        "\n",
        "            logger.info(\"  Continuing training from checkpoint, will skip to saved global_step\")\n",
        "            logger.info(\"  Continuing training from epoch %d\", epochs_trained)\n",
        "            logger.info(\"  Continuing training from global step %d\", global_step)\n",
        "            logger.info(\"  Will skip the first %d steps in the first epoch\", steps_trained_in_current_epoch)\n",
        "        except ValueError:\n",
        "            logger.info(\"  Starting fine-tuning.\")\n",
        "\n",
        "    tr_loss, logging_loss = 0.0, 0.0\n",
        "    model.zero_grad()\n",
        "    mb = master_bar(range(int(args.num_train_epochs)))\n",
        "    # Added here for reproductibility\n",
        "    set_seed(args)\n",
        "\n",
        "    for epoch in mb:\n",
        "        epoch_iterator = progress_bar(train_dataloader, parent=mb)\n",
        "        for step, batch in enumerate(epoch_iterator):\n",
        "            # Skip past any already trained steps if resuming training\n",
        "            if steps_trained_in_current_epoch > 0:\n",
        "                steps_trained_in_current_epoch -= 1\n",
        "                continue\n",
        "\n",
        "            model.train()\n",
        "            batch = tuple(t.to(args.device) for t in batch)\n",
        "\n",
        "            inputs = {\n",
        "                \"input_ids\": batch[0],\n",
        "                \"attention_mask\": batch[1],\n",
        "                \"token_type_ids\": batch[2],\n",
        "                \"start_positions\": batch[3],\n",
        "                \"end_positions\": batch[4],\n",
        "            }\n",
        "\n",
        "            if args.model_type in [\"xlm\", \"roberta\", \"distilbert\", \"distilkobert\", \"xlm-roberta\"]:\n",
        "                del inputs[\"token_type_ids\"]\n",
        "\n",
        "            if args.model_type in [\"xlnet\", \"xlm\"]:\n",
        "                inputs.update({\"cls_index\": batch[5], \"p_mask\": batch[6]})\n",
        "                if args.version_2_with_negative:\n",
        "                    inputs.update({\"is_impossible\": batch[7]})\n",
        "                if hasattr(model, \"config\") and hasattr(model.config, \"lang2id\"):\n",
        "                    inputs.update(\n",
        "                        {\"langs\": (torch.ones(batch[0].shape, dtype=torch.int64) * args.lang_id).to(args.device)}\n",
        "                    )\n",
        "\n",
        "            outputs = model(**inputs)\n",
        "            # model outputs are always tuple in transformers (see doc)\n",
        "            loss = outputs[0]\n",
        "\n",
        "            if args.gradient_accumulation_steps > 1:\n",
        "                loss = loss / args.gradient_accumulation_steps\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            tr_loss += loss.item()\n",
        "            if (step + 1) % args.gradient_accumulation_steps == 0:\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
        "\n",
        "                optimizer.step()\n",
        "                scheduler.step()  # Update learning rate schedule\n",
        "                model.zero_grad()\n",
        "                global_step += 1\n",
        "\n",
        "                # Log metrics\n",
        "                if args.logging_steps > 0 and global_step % args.logging_steps == 0:\n",
        "                    # Only evaluate when single GPU otherwise metrics may not average well\n",
        "                    if args.evaluate_during_training:\n",
        "                        results = evaluation(args, model, tokenizer, global_step=global_step)\n",
        "                        for key in sorted(results.keys()):\n",
        "                            logger.info(\"  %s = %s\", key, str(results[key]))\n",
        "\n",
        "                    logging_loss = tr_loss\n",
        "\n",
        "                # Save model checkpoint\n",
        "                if args.save_steps > 0 and global_step % args.save_steps == 0:\n",
        "                    output_dir = os.path.join(args.output_dir, \"checkpoint-{}\".format(global_step))\n",
        "                    if not os.path.exists(output_dir):\n",
        "                        os.makedirs(output_dir)\n",
        "                    # Take care of distributed/parallel training\n",
        "                    model_to_save = model.module if hasattr(model, \"module\") else model\n",
        "                    model_to_save.save_pretrained(output_dir)\n",
        "                    tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "                    torch.save(args, os.path.join(output_dir, \"training_args.bin\"))\n",
        "                    logger.info(\"Saving model checkpoint to %s\", output_dir)\n",
        "\n",
        "                    if args.save_optimizer:\n",
        "                        torch.save(optimizer.state_dict(), os.path.join(output_dir, \"optimizer.pt\"))\n",
        "                        torch.save(scheduler.state_dict(), os.path.join(output_dir, \"scheduler.pt\"))\n",
        "                        logger.info(\"Saving optimizer and scheduler states to %s\", output_dir)\n",
        "\n",
        "            if args.max_steps > 0 and global_step > args.max_steps:\n",
        "                break\n",
        "\n",
        "        mb.write(\"Epoch {} done\".format(epoch+1))\n",
        "\n",
        "        if args.max_steps > 0 and global_step > args.max_steps:\n",
        "            break\n",
        "\n",
        "    return global_step, tr_loss / global_step\n",
        "\n",
        "\n",
        "def evaluation(args, model, tokenizer, global_step=None):\n",
        "    dataset, examples, features = load_and_cache_examples(args, tokenizer, evaluate=True, output_examples=True)\n",
        "\n",
        "    if not os.path.exists(args.output_dir):\n",
        "        os.makedirs(args.output_dir)\n",
        "\n",
        "    # Note that DistributedSampler samples randomly\n",
        "    eval_sampler = SequentialSampler(dataset)\n",
        "    eval_dataloader = DataLoader(dataset, sampler=eval_sampler, batch_size=args.eval_batch_size)\n",
        "\n",
        "    # Eval!\n",
        "    logger.info(\"***** Running evaluation {} *****\".format(global_step))\n",
        "    logger.info(\"  Num examples = %d\", len(dataset))\n",
        "    logger.info(\"  Batch size = %d\", args.eval_batch_size)\n",
        "\n",
        "    all_results = []\n",
        "    start_time = timeit.default_timer()\n",
        "\n",
        "    for batch in progress_bar(eval_dataloader):\n",
        "        model.eval()\n",
        "        batch = tuple(t.to(args.device) for t in batch)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            inputs = {\n",
        "                \"input_ids\": batch[0],\n",
        "                \"attention_mask\": batch[1],\n",
        "                \"token_type_ids\": batch[2],\n",
        "            }\n",
        "\n",
        "            if args.model_type in [\"xlm\", \"roberta\", \"distilbert\", \"distilkobert\", \"xlm-roberta\"]:\n",
        "                del inputs[\"token_type_ids\"]\n",
        "\n",
        "            example_indices = batch[3]\n",
        "\n",
        "            outputs = model(**inputs)\n",
        "            \n",
        "            start_logits = outputs[0] # bs 512\n",
        "            end_logits = outputs[1] # torch.Tensor\n",
        "            \n",
        "            \n",
        "            # print(len(start_logits)\n",
        "            # print(len(start_logits[0]))\n",
        "            \n",
        "            \n",
        "            \n",
        "            \n",
        "            \n",
        "        # print(len(end_logits))\n",
        "        # print(len(end_logits[0]))\n",
        "        \n",
        "\n",
        "        for i, example_index in enumerate(example_indices):\n",
        "            start_logit = start_logits[i].cpu().tolist()\n",
        "            end_logit = end_logits[i].cpu().tolist()\n",
        "            # start_logits = start_logits[i]\n",
        "            # end_logits = end_logits[i]\n",
        "            # start_logits = start_logits[i]\n",
        "            # end_logits = end_logits[i]\n",
        "            eval_feature = features[example_index.item()]\n",
        "            unique_id = int(eval_feature.unique_id)\n",
        "            result = SquadResult(unique_id, start_logit, end_logit)\n",
        "\n",
        "            all_results.append(result)\n",
        "\n",
        "    evalTime = timeit.default_timer() - start_time\n",
        "    logger.info(\"  Evaluation done in total %f secs (%f sec per example)\", evalTime, evalTime / len(dataset))\n",
        "\n",
        "    # Compute predictions\n",
        "    output_prediction_file = os.path.join(args.output_dir, \"predictions_{}.json\".format(global_step))\n",
        "    output_nbest_file = os.path.join(args.output_dir, \"nbest_predictions_{}.json\".format(global_step))\n",
        "\n",
        "    if args.version_2_with_negative:\n",
        "        output_null_log_odds_file = os.path.join(args.output_dir, \"null_odds_{}.json\".format(global_step))\n",
        "    else:\n",
        "        output_null_log_odds_file = None\n",
        "\n",
        "    predictions = compute_predictions_logits(\n",
        "        examples,\n",
        "        features,\n",
        "        all_results,\n",
        "        args.n_best_size,\n",
        "        args.max_answer_length,\n",
        "        args.do_lower_case,\n",
        "        output_prediction_file,\n",
        "        output_nbest_file,\n",
        "        output_null_log_odds_file,\n",
        "        args.verbose_logging,\n",
        "        args.version_2_with_negative,\n",
        "        args.null_score_diff_threshold,\n",
        "        tokenizer,\n",
        "    )\n",
        "\n",
        "    # Compute the F1 and exact scores.\n",
        "    results = squad_evaluate(examples, predictions)\n",
        "    # Write the result\n",
        "    # Write the evaluation result on file\n",
        "    output_dir = os.path.join(args.output_dir, 'eval')\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    output_eval_file = os.path.join(output_dir, \"eval_result_{}_{}.txt\".format(list(filter(None, args.model_name_or_path.split(\"/\"))).pop(),\n",
        "                                                                               global_step))\n",
        "\n",
        "    with open(output_eval_file, \"w\", encoding='utf-8') as f:\n",
        "        official_eval_results = eval_during_train(args, step=global_step)\n",
        "        results.update(official_eval_results)\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "def load_and_cache_examples(args, tokenizer, evaluate=False, output_examples=False):\n",
        "    # Load data features from cache or dataset file\n",
        "    input_dir = args.data_dir if args.data_dir else \".\"\n",
        "    cached_features_file = os.path.join(\n",
        "        input_dir,\n",
        "        \"cached_{}_{}_{}\".format(\n",
        "            \"dev\" if evaluate else \"train\",\n",
        "            list(filter(None, args.model_name_or_path.split(\"/\"))).pop(),\n",
        "            str(args.max_seq_length),\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    # Init features and dataset from cache if it exists\n",
        "    if os.path.exists(cached_features_file):\n",
        "        logger.info(\"Loading features from cached file %s\", cached_features_file)\n",
        "        features_and_dataset = torch.load(cached_features_file)\n",
        "        features, dataset, examples = (\n",
        "            features_and_dataset[\"features\"],\n",
        "            features_and_dataset[\"dataset\"],\n",
        "            features_and_dataset[\"examples\"],\n",
        "        )\n",
        "    else:\n",
        "        logger.info(\"Creating features from dataset file at %s\", input_dir)\n",
        "\n",
        "        if not args.data_dir and ((evaluate and not args.predict_file) or (not evaluate and not args.train_file)):\n",
        "            try:\n",
        "                import tensorflow_datasets as tfds\n",
        "            except ImportError:\n",
        "                raise ImportError(\"If not data_dir is specified, tensorflow_datasets needs to be installed.\")\n",
        "\n",
        "            if args.version_2_with_negative:\n",
        "                logger.warn(\"tensorflow_datasets does not handle version 2 of SQuAD.\")\n",
        "\n",
        "            tfds_examples = tfds.load(\"squad\")\n",
        "            examples = SquadV1Processor().get_examples_from_dataset(tfds_examples, evaluate=evaluate)\n",
        "        else:\n",
        "            processor = SquadV2Processor() if args.version_2_with_negative else SquadV1Processor()\n",
        "            if evaluate:\n",
        "                examples = processor.get_dev_examples(os.path.join(args.data_dir),\n",
        "                                                      filename=args.predict_file)\n",
        "            else:\n",
        "                examples = processor.get_train_examples(os.path.join(args.data_dir),\n",
        "                                                        filename=args.train_file)\n",
        "\n",
        "        features, dataset = squad_convert_examples_to_features(\n",
        "            examples=examples,\n",
        "            tokenizer=tokenizer,\n",
        "            max_seq_length=args.max_seq_length,\n",
        "            doc_stride=args.doc_stride,\n",
        "            max_query_length=args.max_query_length,\n",
        "            is_training=not evaluate,\n",
        "            return_dataset=\"pt\",\n",
        "            threads=args.threads,\n",
        "        )\n",
        "\n",
        "        logger.info(\"Saving features into cached file %s\", cached_features_file)\n",
        "        torch.save({\"features\": features, \"dataset\": dataset, \"examples\": examples}, cached_features_file)\n",
        "\n",
        "    if output_examples:\n",
        "        return dataset, examples, features\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def main(args):\n",
        "    # Read from config file and make args\n",
        "    logger.info(\"Training/evaluation parameters {}\".format(args))\n",
        "\n",
        "    args.output_dir = os.path.join(args.output_dir)\n",
        "\n",
        "    if args.doc_stride >= args.max_seq_length - args.max_query_length:\n",
        "        logger.warning(\n",
        "            \"WARNING - You've set a doc stride which may be superior to the document length in some \"\n",
        "            \"examples. This could result in errors when building features from the examples. Please reduce the doc \"\n",
        "            \"stride or increase the maximum length to ensure the features are correctly built.\"\n",
        "        )\n",
        "\n",
        "    init_logger()\n",
        "    set_seed(args)\n",
        "\n",
        "    logging.getLogger(\"transformers.data.metrics.squad_metrics\").setLevel(logging.WARN)  # Reduce model loading logs\n",
        "    \n",
        "    \n",
        "\n",
        "    # Load pretrained model and tokenizer\n",
        "    config = CONFIG_CLASSES[args.model_type].from_pretrained(\n",
        "        args.model_name_or_path,\n",
        "    )\n",
        "    tokenizer = TOKENIZER_CLASSES[args.model_type].from_pretrained(\n",
        "        args.model_name_or_path,\n",
        "        do_lower_case=args.do_lower_case,\n",
        "    )\n",
        "    model = MODEL_FOR_QUESTION_ANSWERING[args.model_type].from_pretrained(\n",
        "        args.model_name_or_path,\n",
        "        config=config,\n",
        "    )\n",
        "    # GPU or CPU\n",
        "    args.device = \"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\"\n",
        "    model.to(args.device)\n",
        "\n",
        "    logger.info(\"Training/evaluation parameters %s\", args)\n",
        "\n",
        "    # Training\n",
        "    if args.do_train:\n",
        "        train_dataset = load_and_cache_examples(args, tokenizer, evaluate=False, output_examples=False)\n",
        "        global_step, tr_loss = train(args, train_dataset, model, tokenizer)\n",
        "        logger.info(\" global_step = %s, average loss = %s\", global_step, tr_loss)\n",
        "\n",
        "    # Evaluation - we can ask to evaluate all the checkpoints (sub-directories) in a directory\n",
        "    results = {}\n",
        "    if args.do_eval:\n",
        "        checkpoints = list(\n",
        "            os.path.dirname(c)\n",
        "            for c in sorted(glob.glob(args.output_dir + \"/**/\" + \"pytorch_model.bin\", recursive=True))\n",
        "        )\n",
        "        if not args.eval_all_checkpoints:\n",
        "            checkpoints = checkpoints[-1:]\n",
        "        else:\n",
        "            logging.getLogger(\"transformers.configuration_utils\").setLevel(logging.WARN)  # Reduce model loading logs\n",
        "            logging.getLogger(\"transformers.modeling_utils\").setLevel(logging.WARN)  # Reduce model loading logs\n",
        "\n",
        "        logger.info(\"Evaluate the following checkpoints: %s\", checkpoints)\n",
        "\n",
        "        for checkpoint in checkpoints:\n",
        "            # Reload the model\n",
        "            global_step = checkpoint.split(\"-\")[-1]\n",
        "            model = MODEL_FOR_QUESTION_ANSWERING[args.model_type].from_pretrained(checkpoint)\n",
        "            model.to(args.device)\n",
        "            result = evaluation(args, model, tokenizer, global_step=global_step)\n",
        "            result = dict((k + (\"_{}\".format(global_step) if global_step else \"\"), v) for k, v in result.items())\n",
        "            results.update(result)\n",
        "\n",
        "        output_eval_file = os.path.join(args.output_dir, \"eval_results.txt\")\n",
        "        with open(output_eval_file, \"w\") as f_w:\n",
        "            for key in sorted(results.keys()):\n",
        "                f_w.write(\"{} = {}\\n\".format(key, str(results[key])))\n",
        "                \n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QSGXlI9EF8b7"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOaFLMZuAVPX"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "\n",
        "## 명령행 인터페이스, 사전 훈련된 모델을 로드하여 훈련하고 평가하는데까지 필요한 인자를 사용자 정의 인자로 설정하는 내용을 담고 있음.\n",
        "## 사용법: python example.py --dataset_name squad.json .... \n",
        "## 주피터 환경을 위한 인자 설정법 \n",
        "# \"output_dir\": \"/content/gdrive/My Drive/Colab Notebooks/DFC615/koelectra-small-korquad-ckpt\",\n",
        "\n",
        "    os.chdir('/content/gdrive/MyDrive/Colab Notebooks/DFC615')\n",
        "    gdrive_path = \"/content/gdrive/My Drive/Colab Notebooks/DFC615\"\n",
        "    data_dir = gdrive_path\n",
        "    output_dir = os.path.join(gdrive_path, \"output\")\n",
        "    # train_file = os.path.join(gdrive_path, \"KorQuAD_v1.0_train.json\")\n",
        "    # dev_file = os.path.join(gdrive_path, \"KorQuAD_v1.0_dev.json\")\n",
        "\n",
        "    hyper_para={\n",
        "    \"task\": \"korquad\",\n",
        "    \"data_dir\": data_dir,\n",
        "    \"ckpt_dir\": \"ckpt\",\n",
        "    \"train_file\": \"KorQuAD_v1.0_train.json\",\n",
        "    \"predict_file\": \"KorQuAD_v1.0_dev.json\",\n",
        "    \"threads\": 4,\n",
        "    \"version_2_with_negative\": False,\n",
        "    \"null_score_diff_threshold\": 0.0,\n",
        "    \"max_seq_length\": 512, #don't touch\n",
        "    \"doc_stride\": 128,  #don't touch\n",
        "    \"max_query_length\": 64,  #don't touch\n",
        "    \"max_answer_length\": 30,  #don't touch\n",
        "    \"n_best_size\": 20, #don't touch\n",
        "    \"verbose_logging\": True,\n",
        "    \"overwrite_output_dir\": True,\n",
        "    \"evaluate_during_training\": True,\n",
        "    \"eval_all_checkpoints\": True,\n",
        "    \"save_optimizer\": False,\n",
        "    \"do_lower_case\": False,\n",
        "    \"do_train\": True,\n",
        "    \"do_eval\": True,\n",
        "    \"num_train_epochs\": 10,\n",
        "    \"weight_decay\": 0.0,\n",
        "    \"gradient_accumulation_steps\": 1,\n",
        "    \"adam_epsilon\": 1e-8,\n",
        "    \"warmup_proportion\": 0,\n",
        "    \"max_steps\": -1,\n",
        "    \"max_grad_norm\": 1.0,\n",
        "    \"no_cuda\": False,\n",
        "    \"model_type\": \"koelectra-small-v3\",\n",
        "    \"model_name_or_path\": \"monologg/koelectra-small-v3-discriminator\",\n",
        "    \"output_dir\": output_dir,\n",
        "    \"seed\": 42,\n",
        "    \"train_batch_size\": 32,\n",
        "    \"eval_batch_size\": 64,\n",
        "    \"logging_steps\": 3000,\n",
        "    \"save_steps\": 3000,\n",
        "    \"learning_rate\": 5e-5 #most\n",
        "    }\n",
        "\n",
        "    main(AttrDict(hyper_para))\n",
        "\n",
        "    # hyper_para['learning_rate']\n",
        "    # hyper_para.learning_rate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpQFg7WW249b"
      },
      "source": [
        "- https://github.com/monologg/KoBERT-KorQuAD\n",
        "- https://github.com/Beomi/KcBERT\n",
        "- https://github.com/Beomi/KcBERT-finetune"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "knldPNL8uszZ"
      },
      "source": [
        "## 2.4 Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ONC6b2fD2go"
      },
      "source": [
        "test_file= os.path.join(gdrive_path,'test.json')\n",
        "test_json = json.load(open(test_file,'r', encoding='utf-8'))\n",
        "\n",
        "for sample in test_json['data']:\n",
        "    context = sample['context']\n",
        "    question = sample['question']\n",
        "    print(f'context {context}')\n",
        "    print(f'question {question}')    \n",
        "    print('\\n')\n",
        "    \n",
        "print(len(test_json['data']))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-OlJpiSA0L17"
      },
      "source": [
        "tokenizer = ElectraTokenizer.from_pretrained(\"/content/gdrive/MyDrive/Colab Notebooks/DFC615/output/checkpoint-18000\")\n",
        "model = ElectraForQuestionAnswering.from_pretrained(\"/content/gdrive/MyDrive/Colab Notebooks/DFC615/output/checkpoint-18000\")\n",
        "prediction_qa = pipeline(\"question-answering\", tokenizer=tokenizer, model=model)\n",
        "\n",
        "\n",
        "context_question = {\n",
        "    'question':'스크린이 지구에서 가지러 온 물질은?',\n",
        "    'context':'세 번째 외계인 종족은 스크린이라 알려져 있으며, 스크린은 지구에 뿌려둔 타이베리움을 수확하러 왔다. 그들은 지구에 액화 타이베리움 폭발이 일어나면 동면에서 깨도록 프로그래밍 해놨는데, 사라예보에 있는 노드의최고사원에서 나온 액화 타이베리움 반응을 보고 지구로 오게 된다. 스크린은 자신들이 부닥친 행성의 토착민(인간) 문명이 아직도 강하다는 것과, 전투적인 종족이 있다는것에 놀라, 지구에 군사공격을 시작하게 된다. 그들의 유닛과 건물은 생체공학적이면서, 곤충과 같은 모양을 취하고 있다. 그리고 스크린은 경제와 군사적인 면에 타이베리움과 매우밀접한 관련이 있는데, 그들은 타이베리움을 더 빨리 자라게 하고, 그리고 더 빠른 자원 채광, 그리고 무기에도 사용하고있다. 스크린의 슈퍼무기는 균열 발생기인데, 균열 발생기는 우주공간에 균열을 만들어, 목표지점에 있는 모든 것을우주공간으로 빨아들일 수 있다.'\n",
        "    \n",
        "}\n",
        "\n",
        "answer = prediction_qa(context_question)\n",
        "print(answer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDpiiqpov--1"
      },
      "source": [
        "## 2.5 기말고사 제출 리더보드용 파일생성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8HxecOpSu1kJ"
      },
      "source": [
        "my_answer = {}\n",
        "my_answer['id']=[]\n",
        "my_answer['prediction_text']=[]\n",
        "\n",
        "for idx, sample in enumerate(test_json['data']):\n",
        "    context = sample['context']\n",
        "    question = sample['question']\n",
        "    answer_dict = prediction_qa({'question':question,'context': context})\n",
        "    predict_text = normalize_answer(answer_dict[\"answer\"])\n",
        "    predict_text = normalize_answer2(predict_text)\n",
        "\n",
        "    print(f'context {context}')\n",
        "    print(f'question {question}')\n",
        "    print(f'predict_text {predict_text}')\n",
        "    print('\\n')\n",
        "    \n",
        "    my_answer['id'].append(str(idx + 1))\n",
        "    my_answer['prediction_text'].append(predict_text)\n",
        "    \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0rhkh3xRVRS"
      },
      "source": [
        "csv 파일 생성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9nPwVC_RRTmh"
      },
      "source": [
        "df.to_csv('my_answer.csv', index=False, encoding='utf-8')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EAgG9iMZRZZR"
      },
      "source": [
        "결과 파일을 다시 불러와서 후처리 작업 진행"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCokiZbRR7Gm"
      },
      "source": [
        "!pip install konlpy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0wgFF5GbRdaA"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import json\n",
        "from google.colab import drive\n",
        "from konlpy.tag import Komoran\n",
        "from konlpy.tag import Okt\n",
        "drive.mount('/content/gdrive/') \n",
        "os.chdir('/content/gdrive/My Drive/Colab Notebooks/DFC615') \n",
        "gdrive_path = \"/content/gdrive/My Drive/Colab Notebooks/DFC615\"\n",
        "pd.set_option('max_rows', 99999)\n",
        "pd.set_option('max_columns', 100)\n",
        "result_file= os.path.join(gdrive_path,'my_answer.csv')\n",
        "result = pd.read_csv(result_file)\n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNIZSOWZRmT-"
      },
      "source": [
        "def replace_last(source_string, replace_what, replace_with):\n",
        "    head, _sep, tail = source_string.rpartition(replace_what)\n",
        "    return head + replace_with + tail\n",
        "\n",
        "tokenizer= Komoran()\n",
        "#tokenizer= Okt()\n",
        "\n",
        "pos_table = ['JKS','JKC','JKG','JKO','JKB', 'JKV','JKQ', 'JX', 'JC']\n",
        "\n",
        "result_copy = result.copy()\n",
        "\n",
        "stopwards = ['에서는', '이라는' ,'에겐']\n",
        "\n",
        "\n",
        "\n",
        "for index, row in result_copy.iterrows():\n",
        "    text = row['prediction_text']\n",
        "    #불용어 처리\n",
        "    for word in stopwards:\n",
        "      text = text.replace(word, '')\n",
        "      result_copy.loc[index, 'prediction_text'] =  text\n",
        "\n",
        "    token = tokenizer.pos(text) #형태소 분석\n",
        "    m_text =''\n",
        "    josa = ''\n",
        "    for idx, t in enumerate(token):\n",
        "        if (t[1] in pos_table) and (len(token) -1 == idx):\n",
        "          #token.pop(idx)\n",
        "          josa = t[0]\n",
        "          m_text = replace_last(text, josa, '')  \n",
        "          result_copy.loc[index, 'prediction_text'] =  m_text \n",
        "          break\n",
        "        \n",
        "\n",
        "\n",
        "result_print = pd.concat([result, result_copy], axis=1)\n",
        "result_print # 후처리 작업 후 결과 비교"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-eaT_hYRslh"
      },
      "source": [
        "후처리 작업 완료된 결과 파일 최종 생성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKNMiz0CRq2W"
      },
      "source": [
        "result_copy.to_csv('my_answer_final.csv', index=False, encoding='utf-8')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMXQCXMERrRD"
      },
      "source": [
        "## 기타 시도들(코드 보관용)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j29StLtG5C6-"
      },
      "source": [
        "pd.set_option('max_rows', 99999)\n",
        "pd.set_option('max_columns', 100)\n",
        "df = pd.DataFrame(my_answer)\n",
        "\n",
        "df['prediction_text_mod1'] = df['prediction_text'].replace('[^ a-zA-Z0-9ㄱ-ㅣ가-힣è]','', regex=True)\n",
        "df['prediction_text_mod2'] = df['prediction_text_mod1'].replace('[ ]','', regex=True)\n",
        "print(df)\n",
        "#한자 일어등 제거\n",
        "df1 = df.drop(['prediction_text_mod2','prediction_text'], axis=1)\n",
        "df1.rename(columns={'prediction_text_mod1':'prediction_text'}, inplace=True)\n",
        "\n",
        "#공백까지 제거\n",
        "df2 = df.drop(['prediction_text_mod1','prediction_text'], axis=1)\n",
        "df2.rename(columns={'prediction_text_mod2':'prediction_text'}, inplace=True)\n",
        "\n",
        "#df 원복\n",
        "df.drop(['prediction_text_mod1','prediction_text_mod2'], axis=1, inplace=True)\n",
        "\n",
        "print(df.shape)\n",
        "print(df1.shape)\n",
        "print(df2.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hp1g8yD3kCsR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AYJtPf5v4ZcN"
      },
      "source": [
        "df.to_csv('my_answer.csv', index=False, encoding='utf-8')\n",
        "df1.to_csv('my_answer1.csv', index=False, encoding='utf-8')\n",
        "df2.to_csv('my_answer2.csv', index=False, encoding='utf-8')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XN_d0yBL2zeu"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import json\n",
        "#gdrive_path = \"/content/gdrive/My Drive/Colab Notebooks/DFC615\"\n",
        "\n",
        "result_file= os.path.join(gdrive_path,'my_answer.csv')\n",
        "result = pd.read_csv(result_file)\n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSc4DmdtHR5d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60d68215-b853-4a5d-8d89-3856267ca80f"
      },
      "source": [
        "# !set -x \\\n",
        "# && pip install konlpy \\\n",
        "# && curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh | bash !-x\n",
        "!pip install konlpy"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: konlpy in /usr/local/lib/python3.7/dist-packages (0.5.2)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n",
            "Requirement already satisfied: tweepy>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.19.5)\n",
            "Requirement already satisfied: beautifulsoup4==4.6.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.6.0)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.7/dist-packages (from konlpy) (0.4.4)\n",
            "Requirement already satisfied: JPype1>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.3.0)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (3.7.4.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mB3xqi5JGsGq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "outputId": "1b39da17-297f-49c6-e0a8-c91a69cdc73c"
      },
      "source": [
        "from konlpy.tag import Komoran\n",
        "pd.set_option('max_rows', 99999)\n",
        "tokenizer= Komoran()\n",
        "\n",
        "pos_table = ['JKS','JKC','JKG','JKO','JKB', 'JKV','JKQ', 'JX', 'JC']\n",
        "\n",
        "m_text = ''  \n",
        "\n",
        "      token = tokenizer.pos(text) #형태소 분석    \n",
        "      \n",
        "      for idx, t in enumerate(token):\n",
        "        if (t[1] in pos_table) and (len(token) -1 == idx):\n",
        "          token.pop(idx)\n",
        "          break\n",
        "        m_text += ''.join(t[0])\n",
        "\n",
        "      return m_text\n",
        "\n",
        "\n",
        "for idx, row in result.iterrows():\n",
        "   text = row['prediction_text']\n",
        "   text += 'test'\n",
        "   result.loc[df]\n",
        "   row['prediction_text'] = text\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-96766eaa38a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5139\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5140\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5141\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5143\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'type'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pW0RyM0crqTp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aad33def-60a6-4e3f-abc9-ad0c33d4511c"
      },
      "source": [
        "import re\n",
        "from konlpy.tag import Okt\n",
        "tokenizer = Okt()\n",
        "def text_preprocessing(text,tokenizer):\n",
        "    \n",
        "    stopwords = ['을', '를', '이', '가', '은', '는']\n",
        "    \n",
        "    txt = re.sub('[^가-힣a-z]', ' ', text)\n",
        "    token = tokenizer.morphs(txt)\n",
        "    token = [t for t in token if t not in stopwords]\n",
        "        \n",
        "    return token\n",
        "\n",
        "ex_text = \"이번에 새롭게 개봉한 영화의 배우들은 모두 훌륭한 연기력과 아름다운 목소리를 갖고 있어!!\"\n",
        "example_pre= text_preprocessing(ex_text,tokenizer)\n",
        "example_pre"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['이번',\n",
              " '에',\n",
              " '새롭게',\n",
              " '개봉',\n",
              " '한',\n",
              " '영화',\n",
              " '의',\n",
              " '배우',\n",
              " '들',\n",
              " '모두',\n",
              " '훌륭한',\n",
              " '연기력',\n",
              " '과',\n",
              " '아름다운',\n",
              " '목소리',\n",
              " '갖고',\n",
              " '있어']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJsV3BnJu8I0"
      },
      "source": [
        "result\n",
        "\n",
        "remove_text = 'asdf(asdf) dsfsdfsdf'\n",
        "\n",
        "print(re.sub(r'\\([^)]*\\)', '', remove_text))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WYZQ4rg1xMzy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6758526-0dd3-4777-8c8e-604a61642305"
      },
      "source": [
        "sample1 = '차범근 축구교실 을'\n",
        "tokenizer= Komoran()\n",
        "token = tokenizer.pos(sample1) #형태소 분석\n",
        "print(token)\n",
        "\n",
        "# for t in token[::-1]:\n",
        "#   if t[1] in pos_table and len(token) -1 == idx:\n",
        "#     print(t.index)\n",
        "#     break\n",
        "# print(token)\n",
        "onebok = ''\n",
        "for idx, t in enumerate(token):\n",
        "   #print(idx, t)\n",
        "   print(len(t[1]))\n",
        "   if (t[1] in pos_table) and (len(token) -1 == idx):\n",
        "     print(idx, t) # real index 필요\n",
        "     token.pop(idx)\n",
        "     break\n",
        "\n",
        "   onebok += ''.join(t[0])\n",
        "  \n",
        "\n",
        "print(token)\n",
        "print(onebok)\n",
        "# for t in token:\n",
        "#   if t[1] in pos_table and idx\n",
        "\n",
        "# for t in token :\n",
        "#  print(t)\n",
        "#  if t[1] not in pos_table\n",
        "\n",
        "\n",
        "# for idx, row in result.iterrows():\n",
        "#   text = row['prediction_text']\n",
        "#   token = tokenizer.pos(text) #형태소 분석\n",
        "#   token = [t for t in token if t[1] not in pos_table]\n",
        "#   final = ''\n",
        "#   for t in token:\n",
        "#     final += t[0]\n",
        "#   print(final)\n",
        "  \n"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('차범근', 'NNP'), ('축구', 'NNP'), ('교실', 'NNP'), ('을', 'NNG')]\n",
            "3\n",
            "3\n",
            "3\n",
            "3\n",
            "[('차범근', 'NNP'), ('축구', 'NNP'), ('교실', 'NNP'), ('을', 'NNG')]\n",
            "차범근축구교실을\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JOK6cXp-milc"
      },
      "source": [
        " s = \"가나다-aβbc-123 김家네 김밥clichè\"\n",
        "import unicodedata\n",
        "for c in s:\n",
        "    print(c, unicodedata.name(c))\n",
        "\n",
        "print(s)\n",
        "' '.join(s.split())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5cf2lxIvphwN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}